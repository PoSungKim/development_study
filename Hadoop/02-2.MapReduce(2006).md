# MapReduce(2006)
> 데이터 처리르 위한 프로그래밍 모델
* 하둡은 다양한 언어로 작성된 Map Reduce Program을 구동시킬 수 있으며, 병행성을 고려하여 설계되었기 때문에 누구나 대용량 데이터셋 기반 대규모 데이터 분석 가능

<hr>
<br>


## 분산형으로 확장하기

#### 로컬 파일시스템 >> HDFS 분산 파일시스템으로 확장

<br>

<div align="center">
 <img src="https://user-images.githubusercontent.com/37537227/127882108-de8a93ca-6c29-4b56-8402-ce57dcfe34ba.png"/>
</div>

<br>

### [데이터 흐름]
> MapReduce Job : 클라이언트가 수행하는 작업의 기본 단위 
* Job = Map Task + Reduce Task
  * 각 Task는 YARN을 이용하여 스케줄링되고 클러스터의 여러 노드에서 실행된다

<br>

* 하둡 맵리듀스 잡의 입력 = Input Split
  * 각 Split마다 맵 태스크를 생성하고 스필릿의 각 레코드를 사용자 정의 맵 함수로 처리

<br>

* 전체 입력을 다수의 스플릿으로 나누고, 각 스플릿의 크기는 HDFS 블록의 크기와 같게 하면 효과적 (보통, 128MB이지만, 개별 설정 가능)
  * 이는 Data Locality Optimization을 위한 설정으로, HDFS 블록 크기가 단일 노드에 저장된다고 확신할 수 있는 가장 큰 입력 크기이기 때문
    * Map Task를 위해 필요한 데이터가 로컬에 다 있어야 제일 효과적이라는 의미
  * `클러스터의 중요한 공유 자원인 네트워크 대역폭을 사용하지 않는 방법`이 매우 중요!
  * Map Task의 결과는 HDFS가 로컬이 아닌 로컬 디스크에 저장된다

<br>

* Reduce Task가 복수 개이면, Map Task는 리듀스 수만큼 파티션을 생성하고, 맵의 결과를 각 파티션에 분배한다
  * 정렬되는 파티션 개수와 Reduce Task 개수는 1:1 관계이기 때문에, `Copy`되는 구간이 매우 복잡해지는데, 이 구간을 Shuffle이라고 한다

