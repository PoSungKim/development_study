# Bloomberg BEAP & HAPI
> The Bloomberg Data License Platform feeds enterprise applications with comprehensive data from one source giving front, middle, and back office operations a common, reliable frame of reference.
* Bloomberg Enterprise Access Point (BEAP) and Hypermedia API (HAPI) provide access to the Bloomberg Data License Platform. 

<hr>
<br>

## Basic Information
#### BEAP에 있는 데이터를 HAPI를 통해 받는 형식
#### Request를 보내고, 결과가 오는걸 Monitoring하다가, 도착하면 Notification과 URI를 클릭하여, .bbg 파일을 받는 형식

<br>

### Bloomberg Enterprise Access Point (BEAP)
* Search, explore, discover and acquire subscription (bulk) and custom (PS) datasets through the user- friendly BEAP Web UI.
* Explore sample datasets, different content types (csv, RDF), historical snapshots and useful metadata.

<br>

### Hypermedia API (HAPI)
* HAPI is a REST API that provides full access for you to automate the integration of any content you can request or download through available via BEAP.
* Discover fields, explore bulk datasets and prototype custom datasets in BEAP; integrate them through HAPI.

<br>

### Authentication and Authorization
* Each request to HAPI must include a JSON Web Token (JWT) header, unique to and matching the request. The token must be signed with a valid Data License credential.

<br>
<hr>
<br>

## Extract from Bloomerg
#### Open API 방식과 SFTP 방식 가능

<br>

### Open API 방식

```python
#!/usr/bin/env python
# coding: utf-8
################################################################################
#                          Request a Custom Dataset                            #
################################################################################

################################################################################
# - Import common system-related libraries
import os, sys, traceback, requests, logging, uuid, datetime, collections, math, contextlib, multiprocessing.pool, errno
# Cope with different location of urljoin in python 2 vs 3
try:
    from urllib.parse import urljoin
except ImportError:
    from urlparse import urljoin

# - Import module needed to authorize into BEAP service
from beap_lib.beap_auth import Credentials, BEAPAdapter
# - Import SSEClient component which handles notification protocol
from beap_lib.sseclient import SSEClient
################################################################################

#
class BeapAuth :
    pass

class SseClient:
    pass


class MultipartFileWriter:
    """Representation of a file that can be written in parts."""

    def __init__(self, filename, data_ranges):
        """
        Construct a multipartition file object.
        :param filename: name of the resulting file
        :param data_ranges: byte ranges for multipart download
        """
        self._filename = filename
        self._fp = [None] * len(data_ranges)
        for data_range in data_ranges:
            self._fp[data_range.number] = open(self.get_part_name(data_range.number), 'wb')
            # The following instruction preserves file size (this also enables
            # disk space efficient technique of sparse files on some filesystems)
            self._fp[data_range.number].truncate(data_range.end - data_range.begin)

    def __enter__(self):
        """
        Return instance of this class to be used within 'with' statement
        :return: instance of this class
        """
        return self

    def __exit__(self, exc_type, exc_value, exc_traceback):
        """
        Concatenate file parts in case of successful execution of the indented
        block of code following the with statement and close the inner file
        handler.
        NOTE: in this sample '__exit__' method is used as part of 'with'
        statement in order to ensure that the file is guaranteed to be closed
        :param exc_type: type of the exception
        :param exc_value: value of the exception
        :param exc_traceback: traceback of the exception
        """
        if not exc_type:
            for i in range(1, len(self._fp)):
                self._fp[i].close()
                with open(self.get_part_name(i), 'rb') as filepart:
                    for chunk in filepart:
                        self._fp[0].write(chunk)
                # Remove concatenated file part
                os.remove(self.get_part_name(i))

        for i in range(len(self._fp)):
            if self._fp[i]:
                self._fp[i].close()
                self._fp[i] = None

        # Set fully concatenated file final name in case of successful execution
        # of the indented block of code following the with statement
        if not exc_type:
            os.rename(self.get_part_name(0), self._filename)

    def get_part_name(self, part_number):
        """
        Return file name for the part with specified number.
        :param part_number: number of the file part
        :return: file name
        """
        return self._filename + '.' + str(os.getpid()) + '.part' + str(part_number + 1)

    def write(self, part_number, position, data):
        """
        Write chunk of data to the specified position in the file.
        :param part_number: number of the file part
        :param position: where to write the data
        :param data: the data to write
        """
        if not self._fp[part_number]:
            raise ValueError("Can't write: file is closed.")

        self._fp[part_number].seek(position)
        self._fp[part_number].write(data)


def download_part(session_, url, byte_range, out_file):
    """
    Download the specified range of bytes.
    Note that 'Accept-Encoding: gzip, deflate' header added by default in a
    session to speed up downloading.
    :param session_: http session to be used for file download
    :param url: url of the file to download
    :param byte_range: range of bytes to download
    :param out_file: local file for storing the content
    :return: downloaded file chunk
    """
    headers = {
        'Range': 'bytes={}-{}'.format(byte_range.begin, byte_range.end)
    }
    chunk_size = 1 * 1024 * 1024  # 1 MB

    # LOG.info('Downloading %s', byte_range)

    # Request the specified byte range. Note that we pass "stream=True" in
    # order to disable auto decoding, which would fail because we're loading
    # only a part of the file.
    with session_.get(url, headers=headers, stream=True) as response:
        # Each successful range response must return the 206 "Partial Content"
        # status. You can utilize this fact to distinguish from full responses:
        # if response.status_code == 206:
        #    ...
        response.raise_for_status()
        chunk_pos = 0
        for chunk in response.raw.stream(chunk_size, decode_content=False):
            out_file.write(byte_range.number, chunk_pos, chunk)
            chunk_pos += len(chunk)

    # LOG.info('Downloaded %s', byte_range)

class DataCollector :

    def __init__(self):

        # - Load credentials from *credential.txt* file you have obtained from
        # realpath() : resolve symbolic link first and then return abspath
        self.assets_path = os.path.realpath('./')

        # - Load credentials from *credential.txt* file you have obtained from
        self.credentials_path = os.path.join(self.assets_path, 'ps-credential.txt')
        self.CREDENTIALS = Credentials.from_file(self.credentials_path)

        # - Initialize HTTP session with BEAP auth adapter to set version header and provide a signed JWT for each request.
        self.ADAPTER = BEAPAdapter(self.CREDENTIALS)
        self.SESSION = requests.Session()
        self.SESSION.mount('https://', self.ADAPTER)

        # - Create an SSE session to receive notification when reply is delivered
        self.HOST = 'https://api.bloomberg.com'
        # self.SSE_CLIENT = SSEClient(urljoin(self.HOST, '/eap/notifications/sse'), self.SESSION)
        """
        try:
            self.SSE_CLIENT = SSEClient(urljoin(self.HOST, '/eap/notifications/sse'), self.SESSION)
            # Catch an exception to get full error description with the help of the next
            # GET request
        except requests.exceptions.HTTPError as err:
            self.LOG.error(err)
        """

        # - Set up logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s [%(levelname)-8s] [%(name)s:%(lineno)s]: %(message)s',
        )
        self.LOG = logging.getLogger(__name__)

    def main(self):
        # 1) Test for Connection
        # response         = self.SESSION.get('https://api.bloomberg.com/eap/catalogs/')

        # 1) Set Up Download Directory
        download_path    = os.path.join(self.assets_path, 'downloads')
        output_file_name = "OutputFileName.csv.gz" # = distribution_id + os.extsep + extension_gzip
        output_file_path = os.path.join(download_path, output_file_name)
        try:
            os.makedirs(download_path)
        except OSError as err:
            if err.errno != errno.EEXIST:
                self.LOG.exception('Could not create output directory %s', download_path)
                raise

        # 3) Prepare API Call URL
                           # /eap/catalogs/{catalog_id}/datasets/{dataset_id}/snapshots/{snapshot_id}/distributions/{distribution_id}'
        distribution_path = '/eap/catalogs/bbg/datasets/fields/snapshots/20210810/distributions/fields.csv'
        file_url          = urljoin('https://api.bloomberg.com', distribution_path)

        # 4) Send a HEAD request to fetch the file size without loading the file ("Content-Length" of Header)
        head_response     = self.SESSION.head(file_url, allow_redirects=True)

        if not head_response.ok:
            self.LOG.error("Server returned unsuccessful HTTP code - %s",
                           head_response.status_code)
            sys.exit(1)

        file_size         = int(head_response.headers['Content-Length'])

        ##############################################################################
        # HAPI requires the "Accept-Encoding: gzip" header to be specified for range #
        # requests when downloading distributions, and it must be provided both for  #
        # HEAD and GET methods. That is, you must be prepared to receive compressed  #
        # content in GZIP format, and tell the server about it. Also note that bytes #
        # is the only supported unit.                                                #
        ##############################################################################

        # 5-1 Download Once
        response_ = self.SESSION.get(file_url, stream=True, headers=None)
        response_.raise_for_status()
        with open(output_file_path, 'wb') as out_file:
            for chunk in response_.raw.stream(2048, decode_content=False):
                out_file.write(chunk)






        # 5-2) Compute byte ranges for multipart download by dividing a file to 12
        '''
        file_parts_number = 12
        part_size = int(math.ceil(float(file_size) / file_parts_number))
        # Define supplementary data container
        Range = collections.namedtuple("Range", "number, begin, end")

        ranges = [None] * file_parts_number
        begin = 0

        for file_part_number in range(file_parts_number):
            ranges[file_part_number] = Range(file_part_number, begin, min(begin + part_size, file_size) - 1)
            begin += part_size

        max_workers = 4

        # Using 'contextlib.closing' function instead of regular 'with' statement
        # to provide compatibility with older python versions
        with contextlib.closing(multiprocessing.pool.ThreadPool(max_workers)) as executor:
            with MultipartFileWriter(output_file_path, ranges) as output_file:
                executor.map(
                    lambda p: download_part(*p),
                    ((self.SESSION, file_url, byte_range, output_file) for byte_range in ranges)
                )

        self.LOG.info('File Content-Disposition: %s', head_response.headers['Content-Disposition'])
        self.LOG.info('File Content-Encoding: %s', head_response.headers['Content-Encoding'])
        self.LOG.info('File Content-Length: %s bytes', head_response.headers['Content-Length'])
        self.LOG.info('File Content-Type: %s', head_response.headers['Content-Type'])
        self.LOG.info('File downloaded to: %s', output_file_path)
        '''

        '''
        catalog_id = 'bbg'
        dataset_id = 'fields'
        extension_gzip = 'gz'
        snapshot_id = 'latest'
        distribution_name = 'fields'
        dataset_format = 'csv'

        distribution_id = distribution_name + os.extsep + dataset_format
        distribution_path = '/eap/catalogs/{c}/datasets/{ds}/snapshots/{s}/distributions/{d}'.format(
            c=catalog_id,
            ds=dataset_id,
            s=snapshot_id,
            d=distribution_id
        )
        '''






if __name__ == '__main__':
    try :
        collector = DataCollector()
        collector.main()
    except :
        print(traceback.format_exc())
        sys.exit(1)
    else :
        sys.exit(0)


```
