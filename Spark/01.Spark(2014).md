# Spark (2014)
> 인메모리 기반의 대용량 데이터 고속 처리 엔진으로 범용 분산 클러스터 컴퓨팅 프레임워크
* HDFS + MapReduce (Hadoop) 조합에서 MapReduce보다 100배 가량 더 나은 퍼포먼스를 보여주고 있다

<hr>
<br>

## 기본 개념
#### 분산 메모리 기반 병렬 처리 엔진이라는 점을 항상 염두하여 학습 필요

<br>

### Spark 탄생
* Hadoop MapReduce의 빈번한 `Disk I/O` 처리를 대체하기 위해서 `인메모리` 처리를 지원하는 Spark를 사용하기 시작
  * Spark: 최초 데이터 로드와 최종 결과 저장 시에만 디스크를 사용 
    * 메모리에 분산 저장 및 병렬처리

<br>

### 구성
* 컴포넌트
  * Spark Core
  * Spark Streaming : 실시간 데이터처리 기능 또한 장점
  * Spark SQL : Join 등 SQL related operations 모두 가능 
  * Spark MLlib
  * Spark GraphX

* 특징
  * RDD (Resilient Distributed DataSets) - 분산 데이터 컬렉션
    * 컴퓨팅 클러스터로 분할 가능한 불변적 객체 컬렉션
    * 로컬 환경에서 사용되는 컬렉션으로 보이나, 실제로는 여러 노드에 분산된 데이터를 참조
    * 내부적으로, 병렬처리 프로그래밍으로 변환되어 실행된다
  * Cache in Memory
    * 연산의 결과를 Disk에 다시 적재하는 것이 아니라, Memory에 캐시로 저장함으로써 다음 연산으로 빠르게 이어질 수 있다는 특징이 Spark의 강점
    * RDD를 Cache로서 사용 `val oomLines = lines.filter(l => l.contains("OutOfMemoryError")).cache();
  * OLTP보다는 OLAP용도로 적합
    * 현재까지는 작은 데이터면 굳이 Spark을 사용하지 않아도 되지만, 현대에는 데이터 크기가 지속적으로 크게 증가하는 추세이기 때문에, 미래에는 성능이 개선된 Spark가 OLTP로도 가능할 수도 있다는 내용도 있다
  * HDFS, HBase, Cassandra, S3 등 다양한 Storage와 연동 가능
  * Cluster 유형 (자체 혹은 Cluster Manager와 연동 가능)
    * 로컬 모드 (단일 JVM)
    * Stand-alone 자체 클러스터
    * YARN 클러스터
    * Mesos 클러스터
  * Scala, Python, Java, R 등 다양한 언어 사용 가능
    * 성능 이슈로 인해, Scala(JVML) Python은 섞어서 사용한다고 함
    * 이러한 언어들로 Spark SQL을 실행
  * Spark Shell
    * Spark 클러스터의 대화형 콘솔로, scala로 바로 바로 실행 가능
    * 다만 JVM 1개만 실행되기에 다수의 context를 실행시키지는 못함

<br>
<hr>
<br>

## 구조 
#### Spark API와 Runtime 아키텍처를 통해 분산 프로그래밍으로 실행된다
#### 다만, 이러한 분산 아키텍처로 인해 오버헤드가 있기에 OLTP보다는 OLAP에 더 적합

<br>

### 혁명
* 병렬 처리
* 데이터 분산
* 장애 내성

<br>

### 동작구조
* Driver
  * Spark Context
  * RDD
* Cluster Manager
* Worker Node
  * Executor
  * Task

<br>

### 프로그래밍 모델
* RDD >> Dataframe >> Dataset
* RDD
  * 특징 
    * 새로운 RDD만을 리턴하는 `불변성`
    * 계산 로그를 통한 `복원성`
      * 결국 불변성으로 인해 비순환 그래프 (Acyclic Graph)로 RDD의 변환 과정을 기록 할 수 있기에 문제가 생겨도 다시 복원 가능
    * 여러 JVM에 `분산`
  * 연산자 
    * Transform 변환 연산자 
    * Action 행동 연산자
  * 지연 실행 (Lazy Evaluation)
    * 행동 연산자 호출 전까지 변환 연산자를 실행하지 않는다

<br>

### Spark-Submit
```bash
spark-submit \
  --class <main-class> \
  --master <master-url> \
  --deploy-mode <deploy-mode> \
  --conf <key>=<value> \
  <application-jar> \
  [application-arguments]
```

### 최적화
* 데이터셋의 파티셔닝 + 캐시 유지


<br>
<hr>
<br>

## Lambda Architecture (Batch Layer + Streaming Layer)
#### Batch : 일괄 작업으로 Spark은 일괄처리를 지향함
#### Streaming : 실시간 작업 또한 Spark에서 가능한데, 일괄처리를 지향하는 Spark은 Mini-Batch 개념
#### [Spark 객체 관련 유용 포스팅](https://12bme.tistory.com/432)

<br>

### [SparkContext : SparkCore : RDD ]
```scala
import org.apache.spark.SparkConf 
import org.apache.spark.SparkContext 
import org.apache.spark.Sparkcontext._ 
val conf = new SparkConf()
               .setMaster("local")
               .setAppName("My App") 

val sc = new SparkContext(conf)
```
* RDD 생성을 위한 SparkContext
  * RDD 관련 API를 Spark-Core에서 제공

<br>

### [StreamingContext + DStream + 내장 입력 스트림]
```scala

```
* StereamingContext + DStream
  * 일정 시간 내에 유입된 데이터 블록으로 RDD 구성하여 해당 RDD를 실시간으로 분석하는 형태로 실시간 분석 기능 제공
* 내장 입력 스트림
  * 파일 입력 스트림 : textFileStream, binaryRecordsStream, fileStream, 
  * TCP/IP 소켓 입력 스트림 : socketStream, socketTextStream, 
* 외부 입력 스트림
  * [`카프카`](http://kafka.apache.org/documentation.html#introduction), 플럼, 아마존 Kinesis, 트위터, ZeroMQ, MQTT

<br>

### [SparkSession : SparkSQL : DataFrame + DataSet]
```scala
val spark = SparkSession
            .builder() 
            .appName("MyApp") 
            .master("local[*]") 
            .getOrCreate()
```
* SparkSession 내부에 SparkContext 포함
  * 포함되는 이유는 DataFrame과 DataSet은 결국 RDD의 상위 추상화이기 때문에 내부적으로 RDD 연산이 들어가기에 포함을 해야 한다

<br>

### 카프카 (Kafka)
* Zookeeper, publish-subscribe
