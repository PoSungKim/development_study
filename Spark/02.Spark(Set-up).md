# Spark(Set-up)
> ETL 과정에서 발생하는 Debug 및 Error 로그 기록
* 

<hr>
<br>

## Hadoop & Spark Installation
#### [Spark 공식 다운로드 사이트](https://spark.apache.org/downloads.html)

<br>

### [Terminal]
```zsh
curl --output spark-3.2.0-bin-hadoop3.2.tgz https://dlcdn.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz
tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz

hadoop-3.3.1
```

### [환경변수]
```zsh
export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_241.jdk/Contents/Home
export SPARK_HOME=/Users/posungkim/Desktop/Portfolio/Big_Data_Platform/Spark/spark-3.2.0-bin-hadoop3.2
export HADOOP_HOME=/Users/posungkim/Desktop/Portfolio/Big_Data_Platform/Hadoop/hadoop-3.3.1
export export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"

PATH="$SPARK_HOME/bin:$HADOOP_HOME/bin:$PATH

spark-shell // Spark 실행 확인
```

<br>
<hr>
<br>

## [Intellij IDE]
#### 

<br>

### [Plugin 설치]
* Scala 설치

### [New Project]
```bash
spark-shell

Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 11.0.11)
```
* Scala >> sbt (scala build tool)
* Scala 및 Java 버전은 `spark-shell`을 실행하여 Spark 배너 창에 뜨는 버전에 맞춰주는 것을 추천

### [build.sbt]
```scala
name := "chatbot_spark"

version := "0.1"

scalaVersion := "2.12.5"

val sparkVersion = "3.2.0"

libraryDependencies += "org.apache.spark" %% "spark-core" % sparkVersion
```

### [src/main/scala/com.chatbot_spark/main.scala]
```scala
package com.chatbot_spark

import org.apache.spark.SparkContext

object main extends App {
  println("Hello World");

  val sparkContext = new SparkContext("local", "chatbot_spark");
  val sourceRDD = sparkContext.textFile("/Users/posungkim/Desktop/Portfolio/Big_Data_Platform/Files/words.txt")
  sourceRDD.take(1).foreach(println)

}
```
