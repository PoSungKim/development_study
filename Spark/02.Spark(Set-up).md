# Spark(Set-up)
> RESTful한 서비스를 제공할 수 있는 Spark 서비스 구축
* `Akka Http`와 `Spark-core`를 기반으로 구축

<hr>
<br>

## Hadoop & Spark Installation
#### [Spark 공식 다운로드 사이트](https://spark.apache.org/downloads.html)

<br>

### [Terminal]
```zsh
curl --output spark-3.2.0-bin-hadoop3.2.tgz https://dlcdn.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz
tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz

hadoop-3.3.1
```

<br>

### [.zshrc]
```zsh
export JAVA_HOME=/Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home
export SPARK_HOME=/Users/posungkim/Desktop/Portfolio/Big_Data_Platform/Spark/spark-3.2.0-bin-hadoop3.2
export HADOOP_HOME=/Users/posungkim/Desktop/Portfolio/Big_Data_Platform/Hadoop/hadoop-3.3.1
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"

PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
```
* `source .zshrc` 이후에 `spark-shell`을 통해 Spark 실행 확인

<br>
<hr>
<br>

## Intellij IDE
#### 새 프로젝트 생성 및 실행

<br>

### [Plugin 설치]
* Scala 설치

<br>

### [New Project]
```bash
spark-shell

Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 11.0.11)
```
* Scala >> sbt (scala build tool)
* Scala 및 Java 버전은 `spark-shell`을 실행하여 Spark 배너 창에 뜨는 버전에 맞춰주는 것을 추천

<br>

### [build.sbt]
```scala
name := "chatbot_spark"
version := "0.1"
scalaVersion := "2.12.5"
val sparkVersion = "3.2.0"

libraryDependencies += "org.apache.spark" %% "spark-core" % sparkVersion
```

<br>

### [com.chatbot_spark/main.scala]
```scala
package com.chatbot_spark

import org.apache.spark.SparkContext

object main extends App {
  println("Hello World");

  val sparkContext = new SparkContext("local", "chatbot_spark");
  val sourceRDD = sparkContext.textFile("/Users/posungkim/Desktop/Portfolio/Big_Data_Platform/Files/words.txt")
  sourceRDD.take(1).foreach(println)

}
```
* build.sbt
* src
  * main
    * scala 
      * com.chatbot_spark : package
        * main.scala
    * resources
  * test

<br>
<hr>
<br>

## Akka Http
#### RESTful API로 Spark 서비스를 제공하기 위한 세팅

<br>

### [/src/main/scala/com.chatbot_spark/main.scala]
```scala
final case class User(id: Long, name: String, email: String)

    implicit val actorSystem = ActorSystem(Behaviors.empty, "chatbotSpark")
    implicit val userMarshaller: spray.json.RootJsonFormat[User] = jsonFormat3(User.apply)

    val getUser = get {
      concat(
        path("hello") {
          complete(HttpEntity(ContentTypes.`text/plain(UTF-8)`, "Hello world from scala akka http server!"))
        },
        path("user" / LongNumber) {
          userid => {
            println("get user by id : " + userid)
            userid match {
              case 1 => complete(User(userid, "Testuser", "test@test.com"  + sourceRDD.name))
              case _ => complete(StatusCodes.NotFound)
            }
          }
        }
      )
    }

    val createUser = post {
      path("user") {
        entity(as[User]) {
          user => {
            println("save user")
            complete(User(user.id, "Testuser", "test@test.com"))
          }
        }
      }
    }

    val updateUser = put {
      path("user") {
        entity(as[User]) {
          user => {
            println("update user")
            complete(User(user.id, "Testuser", "test@test.com"))
          }
        }
      }
    }

    val deleteUser = delete {
      path("user" / LongNumber) {
        userid => {
          println(s"user ${userid}")
          complete(User(userid, "Testuser", "test@test.com"))
        }
      }
    }

    val route = cors() {
      concat(getUser, createUser, updateUser, deleteUser)
    }

    val bindFuture = Http().newServerAt("localhost", 8081).bind(route)
```

<br>

### [/src/main/resources/application.conf]
```bash
akka-http-cors {
  allowed-origins = "http://localhost:4200"
  allowed-methods = ["GET", "POST", "PUT", "DELETE", "HEAD", "OPTIONS"]
}

akka {
  loggers = ["akka.event.Logging$DefaultLogger"]
  loglevel = "error"
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
}
```

<br>

### [/build.sbt]
```sbt
name := "com/chatbot_spark"
version := "0.1"
scalaVersion := "2.12.5"
val sparkVersion = "3.2.0"

mainClass := Some("com.chatbot_spark.main")

libraryDependencies ++= Seq (
  "org.apache.spark" %% "spark-core" % sparkVersion,

  "com.typesafe.akka" %% "akka-actor-typed" % "2.6.17" ,
  "com.typesafe.akka" %% "akka-stream" % "2.6.17",
  "com.typesafe.akka" %% "akka-http" % "10.2.7",
  "com.typesafe.akka" %% "akka-http-spray-json" % "10.2.7",
  "ch.megard" %% "akka-http-cors" % "1.1.2"
)

assemblyMergeStrategy in assembly := {
  case PathList("META-INF", xs @ _*) => MergeStrategy.discard
  case PathList("reference.conf") => MergeStrategy.concat
  case x => MergeStrategy.first
}
```

* spark-submit을 했을 때, `no configuration setting found for key akka`가 발생하면, .conf 파일에서 정의해주거나 build.sbt에서 assemblyMergeStrategy를 정의해주면 된다

<br>

### [[StackOverFlow](https://stackoverflow.com/questions/28365000/no-configuration-setting-found-for-key-akka)]

```bash
Akka will read the configuration file from the following location by default:

1) application.conf under root of classpath (including in jar) 
2) manually passed in configuration from ActorSystem("name", config).
3) reference.conf under root of classpath (including in jar)

Please double check your classpath and see if you have a bad classpath reference which indicate a bad root of classpath for akka jars, spray jars, etc.
```

<br>
<hr>
<br>

## Build
#### 로컬 혹은 클러스터에서 .jar 실행

<br>

### [/project/assembly.sbt]
```sbt
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.6")
```
* `sbt assembly` : 명령어로 하나의 jar로 모두 묶을 수 있다
  * `/target/scala-{version}/com/` 디렉토리에서 `{projectName}-assembly-0.1.jar` 파일 확인 가능

<br>

### [Spark-Submit]
```bash
spark-submit \
  --name "RESTful Spark" \
  --class com.chatbot_spark.main \
  --master "local[*]" \
  --deploy-mode client \
  chatbot_spark-assembly-0.1.jar
```
* master
  * local : 1대의 로컬장비에서 실행
    * local : Local 모드에서 Single 코어로 실행
    * local[N] : Local 모드에서 N개 코어로 실행
    * local[*] : Local 모드에서 머신이 갖고 있는 만큼의 코어로 실행

* deploy-mode
  * clien   : spark-submit이 Driver를 자신이 실행되는 머신에서 실행
  * cluster : Cluster의 작업 노드에서 실행되도록 Driver를 전송
    * default : client

<br>

### [Spark Web UI]

<div align="center">
  <img width="80%" src="https://user-images.githubusercontent.com/37537227/142016100-28e6c86e-d48c-42b4-a371-e75a53210fef.png" />
</div>

* Default Port : 4040
