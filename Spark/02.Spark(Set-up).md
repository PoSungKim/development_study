# Spark(Set-up)
> ETL 과정에서 발생하는 Debug 및 Error 로그 기록
* 

<hr>
<br>

## Hadoop & Spark Installation
#### [Spark 공식 다운로드 사이트](https://spark.apache.org/downloads.html)

<br>

### [Terminal]
```zsh
curl --output spark-3.2.0-bin-hadoop3.2.tgz https://dlcdn.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz
tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz

hadoop-3.3.1
```

<br>

### [환경변수]
```zsh
export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_241.jdk/Contents/Home
export SPARK_HOME=/Users/posungkim/Desktop/Portfolio/Big_Data_Platform/Spark/spark-3.2.0-bin-hadoop3.2
export HADOOP_HOME=/Users/posungkim/Desktop/Portfolio/Big_Data_Platform/Hadoop/hadoop-3.3.1
export export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"

PATH="$SPARK_HOME/bin:$HADOOP_HOME/bin:$PATH

spark-shell // Spark 실행 확인
```

<br>
<hr>
<br>

## Intellij IDE
#### 새 프로젝트 생성 및 실행

<br>

### [Plugin 설치]
* Scala 설치

<br>

### [New Project]
```bash
spark-shell

Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 11.0.11)
```
* Scala >> sbt (scala build tool)
* Scala 및 Java 버전은 `spark-shell`을 실행하여 Spark 배너 창에 뜨는 버전에 맞춰주는 것을 추천

<br>

### [build.sbt]
```scala
name := "chatbot_spark"
version := "0.1"
scalaVersion := "2.12.5"
val sparkVersion = "3.2.0"

libraryDependencies += "org.apache.spark" %% "spark-core" % sparkVersion
```

<br>

### [com.chatbot_spark/main.scala]
```scala
package com.chatbot_spark

import org.apache.spark.SparkContext

object main extends App {
  println("Hello World");

  val sparkContext = new SparkContext("local", "chatbot_spark");
  val sourceRDD = sparkContext.textFile("/Users/posungkim/Desktop/Portfolio/Big_Data_Platform/Files/words.txt")
  sourceRDD.take(1).foreach(println)

}
```
* build.sbt
* src
  * main
    * scala 
      * com.chatbot_spark : package
        * main.scala
    * resources
  * test

<br>
<hr>
<br>

## Akka Http
#### RESTful API로 Spark 서비스를 제공하기 위한 세팅

<br>

### [/src/main/resources/application.conf]
```bash
akka-http-cors {
  allowed-origins = "http://localhost:4200"
  allowed-methods = ["GET", "POST", "PUT", "DELETE", "HEAD", "OPTIONS"]
}

akka {
  loggers = ["akka.event.Logging$DefaultLogger"]
  loglevel = "error"
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
}
```

<br>

### [/build.sbt]
```sbt
name := "com/chatbot_spark"
version := "0.1"
scalaVersion := "2.12.5"
val sparkVersion = "3.2.0"

mainClass := Some("com.chatbot_spark.main")

libraryDependencies ++= Seq (
  "org.apache.spark" %% "spark-core" % sparkVersion,

  "com.typesafe.akka" %% "akka-actor-typed" % "2.6.17" ,
  "com.typesafe.akka" %% "akka-stream" % "2.6.17",
  "com.typesafe.akka" %% "akka-http" % "10.2.7",
  "com.typesafe.akka" %% "akka-http-spray-json" % "10.2.7",
  "ch.megard" %% "akka-http-cors" % "1.1.2"
)

assemblyMergeStrategy in assembly := {
  case PathList("META-INF", xs @ _*) => MergeStrategy.discard
  case PathList("reference.conf") => MergeStrategy.concat
  case x => MergeStrategy.first
}
```

* spark-submit을 했을 때, `no configuration setting found for key akka`가 발생하면, .conf 파일에서 정의해주거나 build.sbt에서 assemblyMergeStrategy를 정의해주면 된다

<br>

### [[StackOverFlow](https://stackoverflow.com/questions/28365000/no-configuration-setting-found-for-key-akka)]

```bash
Akka will read the configuration file from the following location by default:

1) application.conf under root of classpath (including in jar) 
2) manually passed in configuration from ActorSystem("name", config).
3) reference.conf under root of classpath (including in jar)

Please double check your classpath and see if you have a bad classpath reference which indicate a bad root of classpath for akka jars, spray jars, etc.
```

<br>
<hr>
<br>

## Build
#### 로컬 혹은 클러스터에서 .jar 실행

<br>

### [/project/assembly.sbt]
```sbt
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.6")
```
* `sbt assembly` : 명령어로 하나의 jar로 모두 묶을 수 있다
  * `/target/scala-{version}/com/` 디렉토리에서 `{projectName}-assembly-0.1.jar` 파일 확인 가능

<br>

### [Spark-Submit]
```bash
spark-submit \
  --master local \
  --deploy-mode client \
  chatbot_spark-assembly-0.1.jar
```
* master
  * 

* deploy-mode
  * 

<br>

### [Spark Web UI]

<div align="center">
  <img width="80%" src="https://user-images.githubusercontent.com/37537227/142016100-28e6c86e-d48c-42b4-a371-e75a53210fef.png" />
</div>

* Default Port : 4040
