# AWS EC2
> IaaS인 EC2 환경에서 설치부터 Config 파일 설정까지 진행
* 클라우드 서비스 혹은 패키지 서비스 등을 통해 클러스터 세팅이 모두 완료되어 있는 환경을 그대로 사용 가능
* 하지만, 아무래도 직접 해보는 것이 학습에 유리 

<hr>
<br>

## 공통 사항
#### 접속 및 기본 설치

<br>

### [EC2 접속]
```bash
ssh -i ${pemDir} ${ec2Address}
```

<br>

### [Git 설치]
```bash
sudo yum install git -y
```

<br>
<hr>
<br>

## WEB 서버 구축 [:80]
#### Apache HTTP, Nginx 모두 가능

<br>

### [WEB 서버 설치 및 실행]
```bash
sudo yum install httpd -y
sudo service httpd start &

ps -ef | grep httpd
```

<br>

### [WEB 서버 HTML/CSS/JS 구축]
```bash
cd /var/www/html
scp -i ${pemDir} ${localDir} {ec2Dir}

sudo scp -r -i "Data_Engineering_Seoul.pem" /Users/posungkim/Desktop/Portfolio/git/chatbot_react/public/* ec2-user@ec2-13-124-198-75.ap-northeast-2.compute.amazonaws.com:/var/www/html

aws s3 cp s3://data-engineering-fintech/WEB/ /var/www/html/ --recursive
```

<br>
<hr>
<br>

## WAS 서버 구축 [:8080]
#### 

<br>

### [Spring 서버 : Tomcat 설치 후 war 파일 실행 혹은 jar 파일 실행]
```bash
java -jar ~/spring/chatbot_spring-0.0.1-SNAPSHOT.jar & >> ~/spring/log.txt
```

<br>
<hr>
<br>

## Kafka 서버 구축 [:9092], Kafka Producer 서버 구축, Kafka Consumer 서버 구축
####

<br>

### [Kafka 서버 : Java, Kafka 설치 후 Zookeeper 실행]

```bash
sudo yum install -y java-1.8.0-openjdk-devel.x86_64

wget https://dlcdn.apache.org/kafka/3.0.0/kafka_2.13-3.0.0.tgz
tar xvf kafka_2.13-3.0.0.tgz  
ln -s kafka_2.13-3.0.0 kafka // symbolic link

cd kafka
pwd

bin/zookeeper-server-start.sh config/zookeeper.properties &    // Zookeeper 실행
bin/kafka-server-start.sh config/server.properties &           // Kafka 실행 (9092)
sudo netstat -anp | egrep "9092|2181"                          // Kafka, Zookeeper 실행 유무 확인

bin/kafka-topics.sh --create --topic twitter --partitions 1 --replication-factor 1 --bootstrap-server localhost:9092  & // twitter 토픽 추가
bin/kafka-topics.sh --list --bootstrap-server localhost:9092   // 현재 토픽 확인

bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic twitter --from-beginning // Consumer 추가
```
* `config/server.properties` 파일 수정 필요!
  * https://minholee93.tistory.com/entry/ERROR-local-producer-cannot-connect-aws-ec2-kafka
* Spring
  * https://ckddn9496.tistory.com/68

<br>

### [Kafka Producer 서버 : Java, Kafka, Logstash 설치 ]

```bash
sudo yum install -y java-1.8.0-openjdk-devel.x86_64

wget https://dlcdn.apache.org/kafka/3.0.0/kafka_2.13-3.0.0.tgz
tar xvf kafka_2.13-3.0.0.tgz  
ln -s kafka_2.13-3.0.0 kafka // symbolic link

bin/kafka-console-producer.sh --topic twitter --bootstrap-server 172.31.40.67:9092 // 172.31.40.67 >> Kafka 내부 IP주소

wget https://artifacts.elastic.co/downloads/logstash/logstash-7.4.0.tar.gz
tar xvzf logstash-7.4.0.tar.gz
ln -s logstash-7.4.0 logstash

vi ~/.bash_profile
export LS_HOME=/home/ec2-user/logstash
PATH=$PATH:$LS_HOME/bin
source ~/.bash_profile
logstash --version

mkdir producer && cd producer
vi producer_test.conf

input {
  twitter {
    consumer_key => "KoxofBvIwdM9zz2JJ9vxg"
    consumer_secret => "kKBOnftLZ6htxvddgmZkzsii17ZeexCIgpIHNoWtE"
    oauth_token => "81761998-2Vu19ZxxFwEyik7XZ4ubG9mIj91wHdbIXdP08fId4"
    oauth_token_secret => "0E6eh4X0eum4NU81LXIKn6MMgH5TAWteL7asT8JxTo"
    keywords => ["news","game","bigdata","遺��숈궛"]
    full_tweet => true
  }
}

output{
  stdout{
    codec => rubydebug  
  }
}

logstash -f producer_test.conf 

cp producer_test.conf producer.conf
vi producer.conf

input {
  twitter {
    consumer_key => "KoxofBvIwdM9zz2JJ9vxg"
    consumer_secret => "kKBOnftLZ6htxvddgmZkzsii17ZeexCIgpIHNoWtE"
    oauth_token => "81761998-2Vu19ZxxFwEyik7XZ4ubG9mIj91wHdbIXdP08fId4"
    oauth_token_secret => "0E6eh4X0eum4NU81LXIKn6MMgH5TAWteL7asT8JxTo"
    keywords => ["news","game","bigdata","遺��숈궛"]
    full_tweet => true
  }
}

output{
  stdout{
    codec => rubydebug  
  }
  kafka {
          bootstrap_servers => "172.31.40.67:9092"
          codec => json{}
          acks => "1"
          topic_id => "twitter"
  }
}
```

<br>

### [Kafka Consumer 서버 : Java, Logstash 설치 ]
```bash
sudo yum install -y java-1.8.0-openjdk-devel.x86_64

wget https://artifacts.elastic.co/downloads/logstash/logstash-7.4.0.tar.gz
tar xvzf logstash-7.4.0.tar.gz
ln -s logstash-7.4.0 logstash

vi ~/.bash_profile
export LS_HOME=/home/ec2-user/logstash
PATH=$PATH:$LS_HOME/bin
source ~/.bash_profile
logstash --version

mkdir consumer && cd consumer

vi consumer.conf

input {
        kafka {
                bootstrap_servers => "172.31.40.67:9092"
                topics => ["twitter"]
                consumer_threads => 1
                decorate_events => true
        }
}

output {
        stdout {
                codec=> rubydebug
        }
}

```

<br>
<hr>
<br>

## Spark 서버 구축 [:4040]
####

<br>

### [Spark Consumer 서버 : ]
```bash
wget https://dlcdn.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz
tar xvzf spark-3.2.0-bin-hadoop3.2.tgz
ln -s spark-3.2.0-bin-hadoop3.2 spark

vi ~/.bash_profile
export SPARK_HOME=/home/ec2-user/spark

PATH=$PATH:$LS_HOME/bin:$SPARK_HOME/bin:$SPARK_HOME/sbin:
export PATH

spark-shell \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.1

spark-submit \
  --class com.chatbot_spark.main \
  --master "local[*]" \
  --deploy-mode client \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0 \
  chatbot_spark-assembly-0.1.jar
```

<br>

### [Batch Read/Write && Real-Time Read/Write]
```bash
val kd = spark.read.format("kafka").option("kafka.bootstrap.servers", "13.209.19.186:9092").option("subscribe", "chatbot").option("startingOffsets","earliest").load()
val response = kd.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)").as[(String, String)]
response.show
response.coalesce(1).write.mode("overwrite").csv("./test.csv")

val kd = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "13.209.19.186:9092").option("subscribe", "chatbot").load()
val response = kd.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)").as[(String, String)]
val stream = response.writeStream.outputMode("append").format("console").start()

http://ec2-54-180-96-220.ap-northeast-2.compute.amazonaws.com:8081/runSparkBatch
```
* https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#deploying

<br>
<hr>
<br>

## HDFS 서버 구축 (의사분산 모드)
#### 

<br>

### [HDFS 서버 : Java, 하둡 설치]
```bash
sudo yum install -y java-1.8.0-openjdk-devel.x86_64
export JAVA_HOME=

tar zxvf hadoop--x.y.z.tar.gz
export HADOOP_HOME=
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

<br>

### [공통 속성 : core-site.xml]
```bash
<?xml version="1.0"?>
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost/</value>
  </property>
</configuration>
```

<br>

### [HDFS 속성 : hdfs-site.xml]
```bash
<?xml version="1.0"?>
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
</configuration>
```

<br>

### [MapReduce 속성 : mapred-site.xml]
```bash
<?xml version="1.0"?>
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
```

<br>

### [Yarn 속성 : yarn-site.xml]
```bash
<?xml version="1.0"?>
<configuration>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>localhost</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuhffle</value>
  </property>
</configuration>
```

<br>

### [SSH 설정]
```bash
sudo apt-get install ssh
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

ssh localhost # 접속 가능 확인
```
* 의사분산 모드는 모든 호스트가 localhost인 특별한 케이스의 완전분산 모드
  * 하둡은 두 모드를 다르게 보지 못하며, 동일한 방식으로 실행시킨다
  * 따라서, 동일 셋업 필요

<br>

### [HDFS 파일시스템 포맷하기]
```bash
hdfs namenode -format
```
* namenode가 메타 정보를 관리하기에 해당 정보를 포맷

<br>

### [데몬 시작]
```bash
start-dfs.sh
start-yarn.sh
mr-jobhistory-daemon.sh start historyserver
```
* 기본 conf 디렉토리가 아닌, 별도의 conf 디렉토리 사용 시의 start-dfs.sh 
  * start-dfs.sh --config path-to-config-directory
  * export HADOOP_CONF_DIR=path-to-config-directory

<br>
<hr>
<br>
